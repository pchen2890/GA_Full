{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = \"\"\"\n",
    "Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
    "\"\"\"\n",
    "\n",
    "ham = \"\"\"\n",
    "Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learn the vocabulary\n",
    "cv.fit([spam, ham])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 2,\n",
       "         0, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 1, 1, 2, 1, 0, 1, 0, 2,\n",
       "         0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 4,\n",
       "         1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 1, 2, 2, 0, 0, 2, 2, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 4,\n",
       "         0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 1,\n",
       "         0, 1, 1, 0, 1, 3, 1, 0, 0, 1, 5, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0, 4, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([spam, ham]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = cv.fit_transform([spam, ham]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3],\n",
       "        [2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape\n",
    "\n",
    "matrix[:, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>of</th>\n",
       "      <th>and</th>\n",
       "      <th>your</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>contact</th>\n",
       "      <th>have</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>euros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   of  and  your  in  is  contact  have  the  this  euros\n",
       "0   4    3     2   2   2        2     2    2     2      2\n",
       "1   4    2     1   1   0        0     0    3     1      0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df  = pd.DataFrame(cv.fit_transform([spam, ham]).todense(),\n",
    "             columns=cv.get_feature_names())\n",
    "\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 3, 3, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2,\n",
       "         2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 3,\n",
       "         1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 8,\n",
       "         1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1,\n",
       "         1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 7, 1, 1, 2, 1, 1, 3, 3, 1, 1, 2, 6, 3]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(matrix, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "easy_text = \"I went to the zoo today. What do you think of that? I bet you hate it! Or maybe you don't\"\n",
    "\n",
    "easy_split_text = [\"I went to the zoo today.\",\n",
    "                   \"What do you think of that?\",\n",
    "                   \"I bet you hate it!\",\n",
    "                   \"Or maybe you don't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I\n",
      "I \n",
      "I w\n",
      "I we\n",
      "I wen\n",
      "I went\n",
      "I went \n",
      "I went t\n",
      "I went to\n",
      "I went to \n",
      "I went to t\n",
      "I went to th\n",
      "I went to the\n",
      "I went to the \n",
      "I went to the z\n",
      "I went to the zo\n",
      "I went to the zoo\n",
      "I went to the zoo \n",
      "I went to the zoo t\n",
      "I went to the zoo to\n",
      "I went to the zoo tod\n",
      "I went to the zoo toda\n",
      "I went to the zoo today\n",
      "\n",
      " \n",
      " W\n",
      " Wh\n",
      " Wha\n",
      " What\n",
      " What \n",
      " What d\n",
      " What do\n",
      " What do \n",
      " What do y\n",
      " What do yo\n",
      " What do you\n",
      " What do you \n",
      " What do you t\n",
      " What do you th\n",
      " What do you thi\n",
      " What do you thin\n",
      " What do you think\n",
      " What do you think \n",
      " What do you think o\n",
      " What do you think of\n",
      " What do you think of \n",
      " What do you think of t\n",
      " What do you think of th\n",
      " What do you think of tha\n",
      " What do you think of that\n",
      "\n",
      " \n",
      " I\n",
      " I \n",
      " I b\n",
      " I be\n",
      " I bet\n",
      " I bet \n",
      " I bet y\n",
      " I bet yo\n",
      " I bet you\n",
      " I bet you \n",
      " I bet you h\n",
      " I bet you ha\n",
      " I bet you hat\n",
      " I bet you hate\n",
      " I bet you hate \n",
      " I bet you hate i\n",
      " I bet you hate it\n",
      "\n",
      " \n",
      " O\n",
      " Or\n",
      " Or \n",
      " Or m\n",
      " Or ma\n",
      " Or may\n",
      " Or mayb\n",
      " Or maybe\n",
      " Or maybe \n",
      " Or maybe y\n",
      " Or maybe yo\n",
      " Or maybe you\n",
      " Or maybe you \n",
      " Or maybe you d\n",
      " Or maybe you do\n",
      " Or maybe you don\n",
      " Or maybe you don'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " ' What do you think of that?',\n",
       " ' I bet you hate it!']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_sentencer(text):\n",
    "    '''take a string called `text` and return\n",
    "    a list of strings, each containing a sentence'''\n",
    "\n",
    "    sentences = []\n",
    "    substring = ''\n",
    "    for c in text:\n",
    "        if c in ('.', '!', '?'):\n",
    "            print substring\n",
    "            sentences.append(substring + c)\n",
    "            substring = ''\n",
    "        else:\n",
    "            print substring\n",
    "            substring += c\n",
    "    return sentences\n",
    "\n",
    "simple_sentencer(easy_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "['I went to the zoo today.',\n",
    " ' What do you think of that?',\n",
    " ' I bet you hate it!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I went to the zoo today.',\n",
       " 'What do you think of that?',\n",
       " 'I bet you hate it!',\n",
       " \"Or maybe you don't\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_detector = PunktSentenceTokenizer()\n",
    "sent_detector.sentences_from_text(easy_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swim\n",
      "Swim\n",
      "Swimmer\n",
      "Swam\n",
      "Scientist\n",
      "Scienc\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('Swimmed')\n",
    "print stemmer.stem('Swimming')\n",
    "print stemmer.stem('Swimmer')\n",
    "print stemmer.stem('Swam')\n",
    "print stemmer.stem('Scientist')\n",
    "print stemmer.stem('Science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'swim'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stemmer.stem(\"generously\")\n",
    "stemmer.stem(\"scientist\")\n",
    "stemmer.stem(\"swam\")\n",
    "stemmer.stem(\"swimming\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer()\n",
    "mm = tv.fit_transform([spam, ham]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 111)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.09761636,  0.09761636,  0.09761636,  0.09761636,  0.09761636,\n",
       "          0.13890968,  0.13890968,  0.20836453,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.06945484,\n",
       "          0.        ,  0.09761636,  0.        ,  0.09761636,  0.09761636,\n",
       "          0.09761636,  0.19523271,  0.        ,  0.        ,  0.09761636,\n",
       "          0.09761636,  0.09761636,  0.09761636,  0.09761636,  0.19523271,\n",
       "          0.19523271,  0.09761636,  0.        ,  0.        ,  0.06945484,\n",
       "          0.        ,  0.09761636,  0.09761636,  0.19523271,  0.06945484,\n",
       "          0.        ,  0.09761636,  0.        ,  0.13890968,  0.        ,\n",
       "          0.06945484,  0.        ,  0.        ,  0.        ,  0.19523271,\n",
       "          0.        ,  0.        ,  0.09761636,  0.        ,  0.        ,\n",
       "          0.09761636,  0.        ,  0.09761636,  0.09761636,  0.        ,\n",
       "          0.        ,  0.09761636,  0.06945484,  0.09761636,  0.09761636,\n",
       "          0.27781937,  0.09761636,  0.06945484,  0.09761636,  0.09761636,\n",
       "          0.09761636,  0.        ,  0.09761636,  0.        ,  0.09761636,\n",
       "          0.09761636,  0.        ,  0.        ,  0.        ,  0.09761636,\n",
       "          0.09761636,  0.09761636,  0.        ,  0.        ,  0.09761636,\n",
       "          0.        ,  0.09761636,  0.        ,  0.09761636,  0.        ,\n",
       "          0.        ,  0.09761636,  0.        ,  0.13890968,  0.13890968,\n",
       "          0.09761636,  0.09761636,  0.        ,  0.13890968,  0.09761636,\n",
       "          0.09761636,  0.        ,  0.09761636,  0.09761636,  0.13890968,\n",
       "          0.13890968,  0.        ,  0.        ,  0.19523271,  0.13890968,\n",
       "          0.13890968],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.06992495,  0.06992495,  0.13984991,  0.09827708,  0.09827708,\n",
       "          0.09827708,  0.09827708,  0.09827708,  0.09827708,  0.06992495,\n",
       "          0.09827708,  0.        ,  0.09827708,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.19655416,  0.09827708,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.09827708,  0.09827708,  0.06992495,\n",
       "          0.19655416,  0.        ,  0.        ,  0.        ,  0.06992495,\n",
       "          0.09827708,  0.        ,  0.09827708,  0.06992495,  0.09827708,\n",
       "          0.06992495,  0.19655416,  0.09827708,  0.09827708,  0.        ,\n",
       "          0.09827708,  0.09827708,  0.        ,  0.09827708,  0.09827708,\n",
       "          0.        ,  0.09827708,  0.        ,  0.        ,  0.09827708,\n",
       "          0.09827708,  0.        ,  0.06992495,  0.        ,  0.        ,\n",
       "          0.27969981,  0.        ,  0.13984991,  0.        ,  0.        ,\n",
       "          0.        ,  0.09827708,  0.        ,  0.09827708,  0.        ,\n",
       "          0.        ,  0.09827708,  0.09827708,  0.09827708,  0.        ,\n",
       "          0.        ,  0.        ,  0.19655416,  0.09827708,  0.        ,\n",
       "          0.19655416,  0.        ,  0.09827708,  0.        ,  0.09827708,\n",
       "          0.09827708,  0.        ,  0.09827708,  0.20977486,  0.06992495,\n",
       "          0.        ,  0.        ,  0.09827708,  0.34962476,  0.        ,\n",
       "          0.        ,  0.19655416,  0.        ,  0.        ,  0.06992495,\n",
       "          0.06992495,  0.09827708,  0.09827708,  0.        ,  0.27969981,\n",
       "          0.06992495]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsienv]",
   "language": "python",
   "name": "conda-env-dsienv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
